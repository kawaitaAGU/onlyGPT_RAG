# ğŸ“ æ§‹æˆ: GitHub ãƒ¬ãƒã‚¸ãƒˆãƒªã«ä»¥ä¸‹3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç½®ãæ§‹æˆ
# - app.py             : Streamlit ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
# - sample.csv         : é¡ä¼¼å•é¡Œæ¤œç´¢ã®ãƒ™ãƒ¼ã‚¹ï¼ˆäº‹å‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
# - requirements.txt   : ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå®šç¾©

# âœ… app.py
import streamlit as st
import pandas as pd
from openai import OpenAI
from PIL import Image
import base64
import io
from pathlib import Path

# --- API ã‚­ãƒ¼å–å¾— ---
if "OPENAI_API_KEY" not in st.secrets:
    st.error("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# --- CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
csv_path = Path("sample.csv")
df = pd.read_csv(csv_path)

# --- Streamlit UI ---
st.set_page_config(page_title="å›½å®¶è©¦é¨“ é¡ä¼¼å•é¡ŒGPTæ¤œç´¢", layout="wide")
st.title("\U0001F4DA å›½å®¶è©¦é¨“å•é¡Œ é¡ä¼¼æ¤œç´¢ï¼‹æ–°ä½œé¡é¡Œç”Ÿæˆï¼ˆGPTã®ã¿ã§æ¤œç´¢ï¼‰")

uploaded_file = st.file_uploader("å›½å®¶è©¦é¨“ã®å•é¡Œç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["jpg", "jpeg", "png"])

if uploaded_file:
    image = Image.open(uploaded_file)
    st.image(image, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ", use_column_width=True)

    with st.spinner("GPTãŒå•é¡Œæ–‡ã‚’èª­ã¿å–ã‚Šä¸­..."):
        image_bytes = io.BytesIO()
        image.save(image_bytes, format='PNG')
        image_bytes.seek(0)
        base64_image = base64.b64encode(image_bytes.read()).decode()

        # ç”»åƒã‹ã‚‰å•é¡Œæ–‡ã‚’æŠ½å‡º
        vision_response = client.chat.completions.create(
            model="gpt-4o-2024-11-20",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å›½å®¶è©¦é¨“OCRã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚ç”»åƒã‹ã‚‰å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": [{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}]}
            ],
            max_tokens=1000
        )
        extracted_question = vision_response.choices[0].message.content.strip()
        st.markdown("### æŠ½å‡ºã•ã‚ŒãŸå•é¡Œæ–‡")
        st.markdown(extracted_question)

    with st.spinner("GPTãŒå‡ºé¡Œé ˜åŸŸã‚’åˆ¤å®šä¸­..."):
        domain_response = client.chat.completions.create(
            model="gpt-4o-2024-11-20",
            messages=[
                {"role": "system", "content": "ä»¥ä¸‹ã®å›½å®¶è©¦é¨“å•é¡Œã¯ã€ã©ã®å‡ºé¡Œé ˜åŸŸï¼ˆä¾‹ï¼šæ­¯å†…ç™‚æ³•ã€è§£å‰–ã€ç†å·¥ã€ç”Ÿç†ã€å£è…”å¤–ç§‘ãªã©ï¼‰ã«å±ã—ã¾ã™ã‹ï¼Ÿ1èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": extracted_question}
            ]
        )
        predicted_domain = domain_response.choices[0].message.content.strip()
        st.success(f"å‡ºé¡Œé ˜åŸŸã®æ¨å®š: {predicted_domain}")

    with st.spinner("GPTãŒcsvå†…ã‹ã‚‰é¡ä¼¼å•é¡Œ10é¡Œã‚’æŠ½å‡ºä¸­..."):
        sample_text = "\n\n".join([f"{i+1}. {row['è¨­å•']}\na. {row['é¸æŠè‚¢a']}\nb. {row['é¸æŠè‚¢b']}\nc. {row['é¸æŠè‚¢c']}\nd. {row['é¸æŠè‚¢d']}\ne. {row['é¸æŠè‚¢e']}" for i, row in df[df['é ˜åŸŸ'].str.contains(predicted_domain, na=False)].head(20).iterrows()])

        similarity_response = client.chat.completions.create(
            model="gpt-4o-2024-11-20",
            messages=[
                {"role": "system", "content": f"ä»¥ä¸‹ã®å›½å®¶è©¦é¨“å•é¡Œãƒªã‚¹ãƒˆã®ä¸­ã‹ã‚‰ã€æ¬¡ã®å•é¡Œã«æ„å‘³çš„ã«æœ€ã‚‚è¿‘ã„10å•ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚\n\n{sample_text}"},
                {"role": "user", "content": extracted_question}
            ],
            max_tokens=2000
        )
        similar_questions = similarity_response.choices[0].message.content.strip()
        st.markdown("### é¡ä¼¼å•é¡Œï¼ˆGPTãŒ10å•é¸å‡ºï¼‰")
        st.markdown(similar_questions)

    with st.spinner("GPTãŒè§£èª¬ã¨æ–°ä½œé¡é¡Œã‚’ç”Ÿæˆä¸­..."):
        final_response = client.chat.completions.create(
            model="gpt-4o-2024-11-20",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ­¯ç§‘å›½å®¶è©¦é¨“ã®æ•™è‚²AIã§ã™ã€‚æŠ½å‡ºã•ã‚ŒãŸå•é¡Œã¨é¡ä¼¼10å•ã‹ã‚‰ã€æ­£è§£ãƒ»è§£èª¬ãƒ»ç†ç”±ã‚’è¨˜è¿°ã—ã€ã•ã‚‰ã«æ–°ä½œé¡é¡Œã‚’3å•ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": f"ã€æœªçŸ¥ã®å•é¡Œã€‘\n{extracted_question}\n\nã€é¡ä¼¼å•é¡Œã€‘\n{similar_questions}"}
            ],
            max_tokens=2000
        )
        st.markdown("### GPTã«ã‚ˆã‚‹è§£èª¬ã¨é¡é¡Œ")
        st.markdown(final_response.choices[0].message.content.strip())

# âœ… requirements.txt
# streamlit
# pandas
# openai
# pillow

# âœ… sample.csv
# ä»»æ„ã®å›½å®¶è©¦é¨“å•é¡Œãƒ‡ãƒ¼ã‚¿ã€‚é ˜åŸŸ,è¨­å•,é¸æŠè‚¢a,b,c,d,e,æ­£è§£ ã‚’å«ã‚€å½¢å¼ã§ç”¨æ„ã—ã¦ãã ã•ã„ã€‚
